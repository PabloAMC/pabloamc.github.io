---
title: How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters
  in the Wild
authors:
- Pablo Antonio Moreno Casares
- Bao Sheng Loe
- John Burden
- Sean hEigeartaigh
- José Hernández-Orallo
date: '2022-01-01'
publishDate: '2024-02-04T20:39:57.242500Z'
publication_types:
- article-journal
publication: '*Proceedings of the 36th AAAI Conference on Artificial Intelligence*'
url_pdf: 'https://ojs.aaai.org/index.php/AAAI/article/view/20466'
featured: true

summary: The abstract highlights limitations in the general-purpose application of new language models like GPT-3 when directly utilized by humans, emphasizing the need for a novel evaluation framework to discern between failures due to capability constraints versus misunderstandings of user intent, ultimately indicating that these models currently lack comprehensive understanding of human commands and are far from achieving widespread general-purpose utility in real-world scenarios.

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Two prompts (in blue) and continuations (in green) generated by GPT-3. The example of the right shows that getting a language model to do what you want requires more than raw capabilities: ‘understanding’ the command is important in making these systems useful and reliable.'
  focal_point: ''
  preview_only: false

abstract: "The new generation of language models is reported to solve some extraordinary tasks the models were never trained for specifically, in few-shot or zero-shot settings. However, these reports usually cherry-pick the tasks, use the best prompts, and unwrap or extract the solutions leniently even if they are followed by nonsensical text. In sum, they are specialised results for one domain, a particular way of using the models and interpreting the results. In this paper, we present a novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters—in the wild. For a useful and safe interaction in these increasingly more common conditions, we need to understand when the model fails because of a lack of capability or a misunderstanding of the user’s intents. Our results indicate that language models such as GPT-3 have limited understanding of the human command; far from becoming general-purpose systems in the wild."
---
